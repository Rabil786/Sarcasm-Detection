{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-29T14:09:46.291514Z","iopub.execute_input":"2021-06-29T14:09:46.291994Z","iopub.status.idle":"2021-06-29T14:09:46.308278Z","shell.execute_reply.started":"2021-06-29T14:09:46.291895Z","shell.execute_reply":"2021-06-29T14:09:46.307084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install chart_studio","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:09:46.310388Z","iopub.execute_input":"2021-06-29T14:09:46.310853Z","iopub.status.idle":"2021-06-29T14:09:52.917027Z","shell.execute_reply.started":"2021-06-29T14:09:46.310807Z","shell.execute_reply":"2021-06-29T14:09:52.915728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Loading Libraries**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport chart_studio.plotly as py\nimport plotly.graph_objects as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\nimport regex as re","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:09:52.919093Z","iopub.execute_input":"2021-06-29T14:09:52.919385Z","iopub.status.idle":"2021-06-29T14:09:52.963061Z","shell.execute_reply.started":"2021-06-29T14:09:52.919355Z","shell.execute_reply":"2021-06-29T14:09:52.962017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Reading the dataset**","metadata":{}},{"cell_type":"code","source":"sar_acc = pd.read_json('/kaggle/input/news-headlines-dataset-for-sarcasm-detection/Sarcasm_Headlines_Dataset_v2.json',lines=True)\nsar_acc['source'] = sar_acc['article_link'].apply(lambda x: re.findall(r'\\w+', x)[2])\nsar_acc.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:09:52.9647Z","iopub.execute_input":"2021-06-29T14:09:52.965033Z","iopub.status.idle":"2021-06-29T14:09:53.518313Z","shell.execute_reply.started":"2021-06-29T14:09:52.964996Z","shell.execute_reply":"2021-06-29T14:09:53.517369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Pie Chart to show number of sarcastic and acclaim headlines**","metadata":{}},{"cell_type":"code","source":"sar_acc_tar = sar_acc['is_sarcastic'].value_counts()\nlabels = ['Acclaim', 'Sarcastic']\nsizes = (np.array((sar_acc_tar / sar_acc_tar.sum())*100))\ncolors = ['#58D68D', '#9B59B6']\n\ntrace = go.Pie(labels=labels, values=sizes, opacity = 0.8, hoverinfo='label+percent',\n               marker=dict(colors=colors, line=dict(color='#FFFFFF', width=2)))\nlayout = go.Layout(\n    title='Sarcastic Vs Acclaim'\n)\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\niplot(fig, filename=\"Sa_Ac\")","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:09:53.519622Z","iopub.execute_input":"2021-06-29T14:09:53.519917Z","iopub.status.idle":"2021-06-29T14:09:53.727507Z","shell.execute_reply.started":"2021-06-29T14:09:53.519888Z","shell.execute_reply":"2021-06-29T14:09:53.726539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Bar graph to show the frequent occuring words**","metadata":{}},{"cell_type":"code","source":"all_words = sar_acc['headline'].str.split(expand=True).unstack().value_counts()\ndata = [go.Bar(\n            x = all_words.index.values[2:50],\n            y = all_words.values[2:50],\n            marker= dict(colorscale='Viridis',\n                         color = all_words.values[2:100]\n                        ),\n            text='Word counts'\n    )]\n\nlayout = go.Layout(\n    title='Frequent Occuring word (unclean) in Headlines'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\niplot(fig, filename='basic-bar')","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:09:53.728802Z","iopub.execute_input":"2021-06-29T14:09:53.729092Z","iopub.status.idle":"2021-06-29T14:09:57.4017Z","shell.execute_reply.started":"2021-06-29T14:09:53.729063Z","shell.execute_reply":"2021-06-29T14:09:57.400636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Tokeninizing**","metadata":{}},{"cell_type":"code","source":"sar_det = sar_acc[sar_acc.is_sarcastic==1]\nsar_det.reset_index(drop=True, inplace=True)\nacc_det = sar_acc[sar_acc.is_sarcastic==0]\nacc_det.reset_index(drop=True, inplace=True)\n\n# Tokenizing the Headlines of Sarcasm\nsar_news = []\nfor rows in range(0, sar_det.shape[0]):\n    head_txt = sar_det.headline[rows]\n    head_txt = head_txt.split(\" \")\n    sar_news.append(head_txt)\n\n#Converting into single list for Sarcasm\nimport itertools\nsar_list = list(itertools.chain(*sar_news))\n\n# Tokenizing the Headlines of Acclaim\nacc_news = []\nfor rows in range(0, acc_det.shape[0]):\n    head_txt = acc_det.headline[rows]\n    head_txt = head_txt.split(\" \")\n    acc_news.append(head_txt)\n    \n#Converting into single list for Acclaim\nacc_list = list(itertools.chain(*acc_news))","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:09:57.403246Z","iopub.execute_input":"2021-06-29T14:09:57.403676Z","iopub.status.idle":"2021-06-29T14:09:57.833605Z","shell.execute_reply.started":"2021-06-29T14:09:57.403632Z","shell.execute_reply":"2021-06-29T14:09:57.832559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Stop words removal**","metadata":{}},{"cell_type":"code","source":"# removing stopwords\nimport nltk\n\nstopwords = nltk.corpus.stopwords.words('english')\nsar_list_restp = [word for word in sar_list if word.lower() not in stopwords]\nacc_list_restp = [word for word in acc_list if word.lower() not in stopwords]\n\nprint(\"Length of original Sarcasm list: {0} words\\n\"\n      \"Length of Sarcasm list after stopwords removal: {1} words\"\n      .format(len(sar_list), len(sar_list_restp)))\n\nprint(\"==\"*46)\n\nprint(\"Length of original Acclaim list: {0} words\\n\"\n      \"Length of Acclaim list after stopwords removal: {1} words\"\n      .format(len(acc_list), len(acc_list_restp)))","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:09:57.836352Z","iopub.execute_input":"2021-06-29T14:09:57.836681Z","iopub.status.idle":"2021-06-29T14:09:59.145714Z","shell.execute_reply.started":"2021-06-29T14:09:57.836649Z","shell.execute_reply":"2021-06-29T14:09:59.144659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data cleaning to get top 30 words**","metadata":{}},{"cell_type":"code","source":"#Data cleaning for getting top 30\nfrom collections import Counter\nsar_cnt = Counter(sar_list_restp)\nacc_cnt = Counter(acc_list_restp)\n\n#Dictonary to Dataframe\nsar_cnt_df = pd.DataFrame(list(sar_cnt.items()), columns = ['Words', 'Freq'])\nsar_cnt_df = sar_cnt_df.sort_values(by=['Freq'], ascending=False)\nacc_cnt_df = pd.DataFrame(list(acc_cnt.items()), columns = ['Words', 'Freq'])\nacc_cnt_df = acc_cnt_df.sort_values(by=['Freq'], ascending=False)\n\n#Top 30\nsar_cnt_df_30 = sar_cnt_df.head(30)\nacc_cnt_df_30 = acc_cnt_df.head(30)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:09:59.1475Z","iopub.execute_input":"2021-06-29T14:09:59.147821Z","iopub.status.idle":"2021-06-29T14:09:59.252429Z","shell.execute_reply.started":"2021-06-29T14:09:59.147793Z","shell.execute_reply":"2021-06-29T14:09:59.251517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Plotting the top 30 Sarcasm Vs Acclaim**","metadata":{}},{"cell_type":"code","source":"#Plotting the top 30 Sarcasm Vs Acclaim\nfrom plotly import tools\nsar_tr  = go.Bar(\n    x=sar_cnt_df_30['Freq'],\n    y=sar_cnt_df_30['Words'],\n    name='Sarcasm',\n    marker=dict(\n        color='rgba(155, 89, 182, 0.6)',\n        line=dict(\n            color='rgba(155, 89, 182, 1.0)',\n            width=.3,\n        )\n    ),\n    orientation='h',\n    opacity=0.6\n)\n\nacc_tr  = go.Bar(\n    x=acc_cnt_df_30['Freq'],\n    y=acc_cnt_df_30['Words'],\n    name='Acclaim',\n    marker=dict(\n        color='rgba(88, 214, 141, 0.6)',\n        line=dict(\n            color='rgba(88, 214, 141, 1.0)',\n            width=.3,\n        )\n    ),\n    orientation='h',\n    opacity=0.6\n)\n\nfig = tools.make_subplots(rows=2, cols=1, subplot_titles=('Top 30 Most occuring words in Sarcasm Headlines',\n                                                          'Top 30 Most occuring words in Acclaim Headlines'))\n\nfig.append_trace(sar_tr, 1, 1)\nfig.append_trace(acc_tr, 2, 1)\n\n\nfig['layout'].update(height=1200, width=800)\n\niplot(fig, filename='sar_vs_acc')","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:09:59.253534Z","iopub.execute_input":"2021-06-29T14:09:59.253832Z","iopub.status.idle":"2021-06-29T14:09:59.356779Z","shell.execute_reply.started":"2021-06-29T14:09:59.253805Z","shell.execute_reply":"2021-06-29T14:09:59.355797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Stemming the tokenized words**","metadata":{}},{"cell_type":"code","source":"# stemming\nstemmer = nltk.stem.SnowballStemmer(\"english\", ignore_stopwords=True)\n\nprint(\"The stemmed form of learning is: {}\".format(stemmer.stem(\"learning\")))\nprint(\"The stemmed form of learns is: {}\".format(stemmer.stem(\"learns\")))\nprint(\"The stemmed form of learn is: {}\".format(stemmer.stem(\"learn\")))\nprint(\"==\"*46)\nprint(\"The stemmed form of leaves is: {}\".format(stemmer.stem(\"leaves\")))\nprint(\"==\"*46)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:09:59.357951Z","iopub.execute_input":"2021-06-29T14:09:59.358221Z","iopub.status.idle":"2021-06-29T14:09:59.36622Z","shell.execute_reply.started":"2021-06-29T14:09:59.358196Z","shell.execute_reply":"2021-06-29T14:09:59.365304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Lemattization of tokenized words**","metadata":{}},{"cell_type":"code","source":"# lemattization\nfrom nltk.stem import WordNetLemmatizer\nlemm = WordNetLemmatizer()\nprint(\"The lemmatized form of leaves is: {}\".format(lemm.lemmatize(\"leaves\")))","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:09:59.36852Z","iopub.execute_input":"2021-06-29T14:09:59.368931Z","iopub.status.idle":"2021-06-29T14:10:01.646304Z","shell.execute_reply.started":"2021-06-29T14:09:59.368891Z","shell.execute_reply":"2021-06-29T14:10:01.645192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Sarcasm and acclaim headlines after Lemmatization**","metadata":{}},{"cell_type":"code","source":"#Sarcasm headline after Lemmatization\nsar_wost_lem = []\nfor batch in sar_news:\n    sar_list_restp = [word for word in batch if word.lower() not in stopwords]\n    lemm = WordNetLemmatizer()\n    sar_list_lemm =  [lemm.lemmatize(word) for word in sar_list_restp]\n    sar_wost_lem.append(sar_list_lemm)\n\n#Acclaim headline after Lemmatization\nacc_wost_lem = []\nfor batch in acc_news:\n    acc_list_restp = [word for word in batch if word.lower() not in stopwords]\n    lemm = WordNetLemmatizer()\n    acc_list_lemm =  [lemm.lemmatize(word) for word in acc_list_restp]\n    acc_wost_lem.append(sar_list_lemm)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:10:01.647579Z","iopub.execute_input":"2021-06-29T14:10:01.647855Z","iopub.status.idle":"2021-06-29T14:10:03.611095Z","shell.execute_reply.started":"2021-06-29T14:10:01.647828Z","shell.execute_reply":"2021-06-29T14:10:03.610155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Formation of bag of words**","metadata":{}},{"cell_type":"code","source":"# bag of words\nfrom sklearn.feature_extraction.text import CountVectorizer\nvec = []\nfor block in sar_wost_lem:\n    vectorizer = CountVectorizer(min_df=0)\n    sentence_transform = vectorizer.fit_transform(block)\n    vec.append(sentence_transform)\n    \nprint(\"The features are:\\n {}\".format(vectorizer.get_feature_names()))\nprint(\"\\nThe vectorized array looks like:\\n {}\".format(sentence_transform.toarray()))","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:10:03.612338Z","iopub.execute_input":"2021-06-29T14:10:03.612605Z","iopub.status.idle":"2021-06-29T14:10:08.419248Z","shell.execute_reply.started":"2021-06-29T14:10:03.612579Z","shell.execute_reply":"2021-06-29T14:10:08.418195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Converting all sarcasm keywords to single list after lemmatization**","metadata":{}},{"cell_type":"code","source":"# Converting all sarcasm keywords to single list after lemmatization\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nsar_list_wd = list(itertools.chain(*sar_wost_lem))\nfrom wordcloud import WordCloud\nsar_cloud = WordCloud(background_color='black',max_words=1000, width = 1800, height = 1000).\\\n                generate(\" \".join(sar_list_wd))\nplt.figure(figsize=(15,15))\nplt.imshow(sar_cloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:10:08.420326Z","iopub.execute_input":"2021-06-29T14:10:08.420621Z","iopub.status.idle":"2021-06-29T14:10:25.73082Z","shell.execute_reply.started":"2021-06-29T14:10:08.420576Z","shell.execute_reply":"2021-06-29T14:10:25.729754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Converting all acclaim keywords to single list after lemmatization**","metadata":{}},{"cell_type":"code","source":"acc_list_wd = list(itertools.chain(*acc_wost_lem))\nacc_cloud = WordCloud(background_color='black',max_words=1000, width = 1800, height = 1000).\\\n                generate(\" \".join(acc_list_wd))\nplt.figure(figsize=(15,15))\nplt.imshow(acc_cloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:10:25.732512Z","iopub.execute_input":"2021-06-29T14:10:25.732927Z","iopub.status.idle":"2021-06-29T14:10:26.929675Z","shell.execute_reply.started":"2021-06-29T14:10:25.732877Z","shell.execute_reply":"2021-06-29T14:10:26.928501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**creating bigrams for the words and plotting a bigram graph**","metadata":{}},{"cell_type":"code","source":"# bigram\n\nsar_wost_lem_df = pd.DataFrame({'sarcasm':sar_wost_lem})\nacc_wost_lem_df = pd.DataFrame({'acclaim':acc_wost_lem})\n\n## custom function for ngram generation ##\ndef generate_ngrams(text, n_gram=1):\n    ngrams = zip(*[text[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n## custom function for horizontal bar chart ##\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"word\"].values[::-1],\n        x=df[\"wordcount\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n#Plotting the Bigram plot\nfrom collections import defaultdict\nfreq_dict = defaultdict(int)\nfor sent in sar_wost_lem_df[\"sarcasm\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\nsar_2 = horizontal_bar_chart(fd_sorted.head(50), '#9B59B6')\n\n\nfreq_dict = defaultdict(int)\nfor sent in acc_wost_lem_df[\"acclaim\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\nacc_2 = horizontal_bar_chart(fd_sorted.head(50), '#58D68D')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n                          subplot_titles=[\"Frequent bigrams of Sarcasm Headlines\", \n                                          \"Frequent bigrams of Acclaim Headlines\"])\nfig.append_trace(sar_2, 1, 1)\nfig.append_trace(acc_2, 1, 2)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Bigram Plots Sarcasm Vs Acclaim after removing Stopwords\")\niplot(fig, filename='word-plots')","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:10:26.931356Z","iopub.execute_input":"2021-06-29T14:10:26.931795Z","iopub.status.idle":"2021-06-29T14:10:27.245369Z","shell.execute_reply.started":"2021-06-29T14:10:26.931751Z","shell.execute_reply":"2021-06-29T14:10:27.244358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Making trigrams of the words and plotting the graph**","metadata":{}},{"cell_type":"code","source":"#Plotting the Trigram plot\nfrom collections import defaultdict\nfreq_dict = defaultdict(int)\nfor sent in sar_wost_lem_df[\"sarcasm\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\nsar_2 = horizontal_bar_chart(fd_sorted.head(50), '#9B59B6')\n\n\nfreq_dict = defaultdict(int)\nfor sent in acc_wost_lem_df[\"acclaim\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\nacc_2 = horizontal_bar_chart(fd_sorted.head(50), '#58D68D')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n                          subplot_titles=[\"Frequent Trigrams of Sarcasm Headlines\", \n                                          \"Frequent Trigrams of Acclaim Headlines\"])\nfig.append_trace(sar_2, 1, 1)\nfig.append_trace(acc_2, 1, 2)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Trigram Plots Sarcasm Vs Acclaim after removing Stopwords\")\niplot(fig, filename='word-plots')","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:10:27.247077Z","iopub.execute_input":"2021-06-29T14:10:27.247465Z","iopub.status.idle":"2021-06-29T14:10:27.525789Z","shell.execute_reply.started":"2021-06-29T14:10:27.247425Z","shell.execute_reply":"2021-06-29T14:10:27.524821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from wordcloud import STOPWORDS\nimport string\n## Number of words in the text ##\nsar_acc[\"num_words\"] = sar_acc[\"headline\"].apply(lambda x: len(str(x).split()))\n\n## Number of unique words in the text ##\nsar_acc[\"num_unique_words\"] = sar_acc[\"headline\"].apply(lambda x: len(set(str(x).split())))\n\n## Number of characters in the text ##\nsar_acc[\"num_chars\"] = sar_acc[\"headline\"].apply(lambda x: len(str(x)))\n\n## Number of stopwords in the text ##\nsar_acc[\"num_stopwords\"] = sar_acc[\"headline\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n\n## Number of punctuations in the text ##\nsar_acc[\"num_punctuations\"] =sar_acc['headline'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n\n## Number of title case words in the text ##\nsar_acc[\"num_words_upper\"] = sar_acc[\"headline\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\n## Number of title case words in the text ##\nsar_acc[\"num_words_title\"] = sar_acc[\"headline\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n\n## Average length of the words in the text ##\nsar_acc[\"mean_word_len\"] = sar_acc[\"headline\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:10:27.526935Z","iopub.execute_input":"2021-06-29T14:10:27.52721Z","iopub.status.idle":"2021-06-29T14:10:28.678751Z","shell.execute_reply.started":"2021-06-29T14:10:27.527181Z","shell.execute_reply":"2021-06-29T14:10:28.677835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Truncate some extreme values for better visuals ##\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\n\nsar_acc['num_words'].loc[sar_acc['num_words']>60] = 60 #truncation for better visuals\nsar_acc['num_punctuations'].loc[sar_acc['num_punctuations']>10] = 10 #truncation for better visuals\nsar_acc['num_chars'].loc[sar_acc['num_chars']>350] = 350 #truncation for better visuals\n\nsar_acc['num_words'].loc[sar_acc['num_words']>60] = 60 #truncation for better visuals\nsar_acc['num_punctuations'].loc[sar_acc['num_punctuations']>10] = 10 #truncation for better visuals\nsar_acc['num_chars'].loc[sar_acc['num_chars']>350] = 350 #truncation for better visuals\n\nf, axes = plt.subplots(3, 1, figsize=(10,20))\nsns.boxplot(x='is_sarcastic', y='num_words', data=sar_acc, ax=axes[0])\naxes[0].set_xlabel('is_sarcastic', fontsize=12)\naxes[0].set_title(\"Number of words in each class\", fontsize=15)\n\nsns.boxplot(x='is_sarcastic', y='num_chars', data=sar_acc, ax=axes[1])\naxes[1].set_xlabel('is_sarcastic', fontsize=12)\naxes[1].set_title(\"Number of characters in each class\", fontsize=15)\n\nsns.boxplot(x='is_sarcastic', y='num_punctuations', data=sar_acc, ax=axes[2])\naxes[2].set_xlabel('is_sarcastic', fontsize=12)\n#plt.ylabel('Number of punctuations in text', fontsize=12)\naxes[2].set_title(\"Number of punctuations in each class\", fontsize=15)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:10:28.680185Z","iopub.execute_input":"2021-06-29T14:10:28.680471Z","iopub.status.idle":"2021-06-29T14:10:29.156799Z","shell.execute_reply.started":"2021-06-29T14:10:28.680444Z","shell.execute_reply":"2021-06-29T14:10:29.155797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preperation","metadata":{}},{"cell_type":"code","source":"#Getting X and Y ready\nfrom sklearn.preprocessing import LabelEncoder\nX = sar_acc.headline\nY = sar_acc.is_sarcastic\nle = LabelEncoder()\nY = le.fit_transform(Y)\nY = Y.reshape(-1,1)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:10:29.157891Z","iopub.execute_input":"2021-06-29T14:10:29.158163Z","iopub.status.idle":"2021-06-29T14:10:29.163643Z","shell.execute_reply.started":"2021-06-29T14:10:29.158137Z","shell.execute_reply":"2021-06-29T14:10:29.162847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2. Split into Training and Test data\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:10:29.164557Z","iopub.execute_input":"2021-06-29T14:10:29.164863Z","iopub.status.idle":"2021-06-29T14:10:29.178948Z","shell.execute_reply.started":"2021-06-29T14:10:29.164833Z","shell.execute_reply":"2021-06-29T14:10:29.177932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 3. Processing the data for the model\n\n# Tokenize the data and convert the text to sequences.\n# Add padding to ensure that all the sequences have the same shape.\n# There are many ways of taking the max_len and here an arbitrary length of 150 is chosen\n\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nmax_words = 1000\nmax_len = 150\ntok = Tokenizer(num_words=max_words)\ntok.fit_on_texts(X_train)\nsequences = tok.texts_to_sequences(X_train)\nsequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:10:29.183102Z","iopub.execute_input":"2021-06-29T14:10:29.183458Z","iopub.status.idle":"2021-06-29T14:10:31.99297Z","shell.execute_reply.started":"2021-06-29T14:10:29.183419Z","shell.execute_reply":"2021-06-29T14:10:31.992009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RNN","metadata":{}},{"cell_type":"code","source":"# importing the functions from keras library \nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\nfrom keras.optimizers import RMSprop\nfrom keras.models import Model\n\n# defining the RNN function \ndef RNN():\n    inputs = Input(name='inputs',shape=[max_len])\n#     adding words to the layer of NN\n    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n#     adding LTSM to layer\n    layer = LSTM(64)(layer)\n#     adding dense layer\n    layer = Dense(256,name='FC1')(layer)\n#     adding the relu activation function\n    layer = Activation('relu')(layer)\n#     adding dropout pf 20%\n    layer = Dropout(0.2)(layer)\n#     adding the dense layer\n    layer = Dense(1,name='out_layer')(layer)\n#     adding sigmoid activation function\n    layer = Activation('sigmoid')(layer)\n#     initializing the model of RNN based on inputs and layers\n    model = Model(inputs=inputs,outputs=layer)\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:10:31.995405Z","iopub.execute_input":"2021-06-29T14:10:31.995846Z","iopub.status.idle":"2021-06-29T14:10:32.003567Z","shell.execute_reply.started":"2021-06-29T14:10:31.995804Z","shell.execute_reply":"2021-06-29T14:10:32.002873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calling the model of RNN\nmodel = RNN()\n# generating the summary of the model formed\nmodel.summary()\n# compiling the model and assigning loss function and optimizer\nmodel.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:10:32.004489Z","iopub.execute_input":"2021-06-29T14:10:32.004798Z","iopub.status.idle":"2021-06-29T14:10:32.306075Z","shell.execute_reply.started":"2021-06-29T14:10:32.004771Z","shell.execute_reply":"2021-06-29T14:10:32.305084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import EarlyStopping\n# fitting the model\nmodel.fit(sequences_matrix,Y_train,batch_size=100,epochs=5,\n          validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:10:32.307384Z","iopub.execute_input":"2021-06-29T14:10:32.307713Z","iopub.status.idle":"2021-06-29T14:11:40.993749Z","shell.execute_reply.started":"2021-06-29T14:10:32.307684Z","shell.execute_reply":"2021-06-29T14:11:40.992712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating test sequence from text\ntest_sequences = tok.texts_to_sequences(X_test)\n# creating test sequence matrix using above created test sequence\ntest_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:11:40.995192Z","iopub.execute_input":"2021-06-29T14:11:40.99579Z","iopub.status.idle":"2021-06-29T14:11:41.136134Z","shell.execute_reply.started":"2021-06-29T14:11:40.995754Z","shell.execute_reply":"2021-06-29T14:11:41.135255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initializing accuracy matrix to store accuracies of all the models and compare them\naccuracy = {}","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:11:41.137392Z","iopub.execute_input":"2021-06-29T14:11:41.137931Z","iopub.status.idle":"2021-06-29T14:11:41.14216Z","shell.execute_reply.started":"2021-06-29T14:11:41.137897Z","shell.execute_reply":"2021-06-29T14:11:41.141032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluation of RNN model\naccr = model.evaluate(test_sequences_matrix,Y_test)\n# printing the loss and accuracy of our model\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n# storing the model name and accuracy in accuracy dictionary\naccuracy.update({\"RNN\":accr[1]})","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:11:41.143674Z","iopub.execute_input":"2021-06-29T14:11:41.143996Z","iopub.status.idle":"2021-06-29T14:11:45.215356Z","shell.execute_reply.started":"2021-06-29T14:11:41.143956Z","shell.execute_reply":"2021-06-29T14:11:45.214345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Scaling","metadata":{}},{"cell_type":"code","source":"# min max scaling for features\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n# feature scaling on training matrix\nprint(scaler.fit(sequences_matrix))\nscaler.transform(sequences_matrix)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:11:45.216803Z","iopub.execute_input":"2021-06-29T14:11:45.217121Z","iopub.status.idle":"2021-06-29T14:11:45.308324Z","shell.execute_reply.started":"2021-06-29T14:11:45.21709Z","shell.execute_reply":"2021-06-29T14:11:45.30731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# min max scaling on testing matrix\nprint(scaler.fit(test_sequences_matrix))\nscaler.transform(test_sequences_matrix)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:11:45.309694Z","iopub.execute_input":"2021-06-29T14:11:45.310015Z","iopub.status.idle":"2021-06-29T14:11:45.331787Z","shell.execute_reply.started":"2021-06-29T14:11:45.309978Z","shell.execute_reply":"2021-06-29T14:11:45.330715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"# importing libraries for random forest classifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:11:45.33312Z","iopub.execute_input":"2021-06-29T14:11:45.33353Z","iopub.status.idle":"2021-06-29T14:11:45.373259Z","shell.execute_reply.started":"2021-06-29T14:11:45.333489Z","shell.execute_reply":"2021-06-29T14:11:45.372334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reshaping the ytrain and ytest \nY_train = np.ravel(Y_train) \nY_test = np.ravel(Y_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:11:45.375058Z","iopub.execute_input":"2021-06-29T14:11:45.37549Z","iopub.status.idle":"2021-06-29T14:11:45.382017Z","shell.execute_reply.started":"2021-06-29T14:11:45.375443Z","shell.execute_reply":"2021-06-29T14:11:45.380958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initializing randomforestclassifier model with 500 estimators\nRF = RandomForestClassifier(n_estimators=500)\n# fitting the model\nRF.fit(sequences_matrix,Y_train)\n# predicting the values from RFC trained model\nRF_pred = RF.predict(test_sequences_matrix)\n# checking the accuracy of model\naccr = metrics.accuracy_score(Y_test, RF_pred)\nprint(f\"Accuracy of Random Forest Classifier is: {accr}\")\naccuracy.update({\"Random Forest Classifier\":accr})","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:11:45.383381Z","iopub.execute_input":"2021-06-29T14:11:45.383804Z","iopub.status.idle":"2021-06-29T14:12:11.686901Z","shell.execute_reply.started":"2021-06-29T14:11:45.383733Z","shell.execute_reply":"2021-06-29T14:12:11.685789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(Y_test, RF_pred))","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:12:11.68841Z","iopub.execute_input":"2021-06-29T14:12:11.689106Z","iopub.status.idle":"2021-06-29T14:12:11.711054Z","shell.execute_reply.started":"2021-06-29T14:12:11.689061Z","shell.execute_reply":"2021-06-29T14:12:11.709819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decision Tree Classifier","metadata":{}},{"cell_type":"code","source":"# importing DecisionTreeClassifier from sklearn library\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:12:11.712583Z","iopub.execute_input":"2021-06-29T14:12:11.71302Z","iopub.status.idle":"2021-06-29T14:12:11.718258Z","shell.execute_reply.started":"2021-06-29T14:12:11.712979Z","shell.execute_reply":"2021-06-29T14:12:11.717109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initializing the DecisionTreeClassifier model with gini criterion\nDT = DecisionTreeClassifier(criterion = 'gini')\n# fitting the model\nDT = DT.fit(sequences_matrix,Y_train)\n# predicting values from testing dataset\nDT_pred = DT.predict(test_sequences_matrix)\n# calculating the accuracy of model\naccr = metrics.accuracy_score(Y_test, DT_pred)\nprint(f\"Accuracy of Decision Tree Classifier is: {accr}\")\naccuracy.update({\"Decision Tree Classifier\":accr})","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:12:11.719882Z","iopub.execute_input":"2021-06-29T14:12:11.720304Z","iopub.status.idle":"2021-06-29T14:12:12.026482Z","shell.execute_reply.started":"2021-06-29T14:12:11.720263Z","shell.execute_reply":"2021-06-29T14:12:12.025423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(Y_test, DT_pred))","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:12:12.027866Z","iopub.execute_input":"2021-06-29T14:12:12.028297Z","iopub.status.idle":"2021-06-29T14:12:12.053612Z","shell.execute_reply.started":"2021-06-29T14:12:12.028252Z","shell.execute_reply":"2021-06-29T14:12:12.052636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Support Vector Classifier","metadata":{}},{"cell_type":"code","source":"# importing support vector classifier model from sklearn\nfrom sklearn.svm import SVC","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:12:12.054868Z","iopub.execute_input":"2021-06-29T14:12:12.05515Z","iopub.status.idle":"2021-06-29T14:12:12.059398Z","shell.execute_reply.started":"2021-06-29T14:12:12.055124Z","shell.execute_reply":"2021-06-29T14:12:12.058314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initializing the model\nsvc = SVC(C =  0.9, kernel = 'rbf', coef0 = 4)\n# fitting the model\nsvc.fit(sequences_matrix,Y_train)\n# predicting values from test dataset\nsvc_pred = svc.predict(test_sequences_matrix)\n# calculating the accuracy\naccr = metrics.accuracy_score(Y_test, svc_pred)\nprint(f\"Accuracy of Support Vector Classifier is: {accr}\")\naccuracy.update({\"Support Vector Classifier\":accr})","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:12:12.060751Z","iopub.execute_input":"2021-06-29T14:12:12.061064Z","iopub.status.idle":"2021-06-29T14:14:31.182866Z","shell.execute_reply.started":"2021-06-29T14:12:12.061027Z","shell.execute_reply":"2021-06-29T14:14:31.18185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(Y_test, svc_pred))","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:14:31.184189Z","iopub.execute_input":"2021-06-29T14:14:31.184604Z","iopub.status.idle":"2021-06-29T14:14:31.204942Z","shell.execute_reply.started":"2021-06-29T14:14:31.184546Z","shell.execute_reply":"2021-06-29T14:14:31.203941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Accuracy Comparision","metadata":{}},{"cell_type":"code","source":"# sorting the accuracy dicrionary in descending order based on accuracy\naccuracy_sorted = {k:accuracy[k] for k in sorted(accuracy, key=accuracy.get, reverse = True)}\n# printing the accuracy of each model\nprint(\"Accuracy of the above four models are:\\n\")\nfor key,value in accuracy_sorted.items():\n    print(f\"accuracy of {key} model is {value}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:14:31.207888Z","iopub.execute_input":"2021-06-29T14:14:31.208178Z","iopub.status.idle":"2021-06-29T14:14:31.214697Z","shell.execute_reply.started":"2021-06-29T14:14:31.208152Z","shell.execute_reply":"2021-06-29T14:14:31.213495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"The highest accuracy we got is from {list(accuracy_sorted.keys())[0]} with accuracy {list(accuracy_sorted.values())[0]}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:14:31.216201Z","iopub.execute_input":"2021-06-29T14:14:31.216611Z","iopub.status.idle":"2021-06-29T14:14:31.227034Z","shell.execute_reply.started":"2021-06-29T14:14:31.216554Z","shell.execute_reply":"2021-06-29T14:14:31.226036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Principal Component Plot ","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(2)\nX_projected = pca.fit_transform(sequences_matrix)\nx1 = X_projected[:,0]\nx2 = X_projected[:,1]","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:14:31.228135Z","iopub.execute_input":"2021-06-29T14:14:31.228396Z","iopub.status.idle":"2021-06-29T14:14:31.643022Z","shell.execute_reply.started":"2021-06-29T14:14:31.22837Z","shell.execute_reply":"2021-06-29T14:14:31.641836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plotting the principle component graph\nfig = plt.figure\nplt.scatter(x1,x2,c=Y_train,alpha=0.8,cmap='cividis')\nplt.xlabel(\"Principal Component 1\")\nplt.ylabel(\"Principal Component 2\")\nplt.colorbar()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:14:31.644786Z","iopub.execute_input":"2021-06-29T14:14:31.645194Z","iopub.status.idle":"2021-06-29T14:14:32.338445Z","shell.execute_reply.started":"2021-06-29T14:14:31.645151Z","shell.execute_reply":"2021-06-29T14:14:32.337509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Classification of User Text into Sarcastic or Non-Sarcastic**","metadata":{}},{"cell_type":"code","source":"def user_text_processing(user_text):\n    user_text = user_text.split()\n    user_text = [word.lower() for word in user_text if word not in stopwords]\n    user_text = [lemm.lemmatize(word) for word in user_text]\n    user_text\n\n    # converting user text to sequence\n    user_seq = np.array(user_text)\n    user_seq = tok.texts_to_sequences(user_seq)\n    user_seq = sequence.pad_sequences(user_seq,maxlen=max_len)\n    \n    # min max scaling\n    scaler.fit(user_seq)\n    scaler.transform(user_seq)\n    return user_seq","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:14:32.339792Z","iopub.execute_input":"2021-06-29T14:14:32.340099Z","iopub.status.idle":"2021-06-29T14:14:32.345876Z","shell.execute_reply.started":"2021-06-29T14:14:32.340052Z","shell.execute_reply":"2021-06-29T14:14:32.345201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_sarcasm(user_seq):\n#     prediction\n    prob = model.predict(user_seq)\n    probability = np.mean(prob, axis=0)\n#     print(prob)\n    if probability > 0.5:\n        return(\"Sarcastic\")\n    elif probability < 0.5:\n        return(\"Not Sarcastic\")\n    elif probability == 0.5:\n        return(\"Neutral\")","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:14:32.346904Z","iopub.execute_input":"2021-06-29T14:14:32.347323Z","iopub.status.idle":"2021-06-29T14:14:32.363168Z","shell.execute_reply.started":"2021-06-29T14:14:32.34728Z","shell.execute_reply":"2021-06-29T14:14:32.362248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# user input and pre-processing\nuser_text = 'state population to double by 2040, babies to blame'\nuser_seq = user_text_processing(user_text)\nuser_seq\nprediction = predict_sarcasm(user_seq)\nprint(f\"Sentence '{user_text}' is of '{prediction}' nature\")","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:14:32.364375Z","iopub.execute_input":"2021-06-29T14:14:32.364715Z","iopub.status.idle":"2021-06-29T14:14:32.790127Z","shell.execute_reply.started":"2021-06-29T14:14:32.364686Z","shell.execute_reply":"2021-06-29T14:14:32.788984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Saving the models","metadata":{}},{"cell_type":"code","source":"# Saving Model\nimport joblib\njoblib.dump(RF, \"./RF_model.joblib\")\n\nfrom keras.models import load_model\nmodel.save('RNN_model.h5')\n\nimport pickle\nwith open('tokenizer.pickle', 'wb') as handle:\n    pickle.dump(tok, handle, protocol=pickle.HIGHEST_PROTOCOL)\n    \nfrom keras.models import model_from_yaml\nmodel_yaml = model.to_yaml()\nwith open(\"model.yaml\", \"w\") as yaml_file:\n    yaml_file.write(model_yaml)\n# serialize weights to HDF5\nmodel.save_weights(\"RNN_model.h5\")\nprint(\"Saved model to disk\")","metadata":{"execution":{"iopub.status.busy":"2021-06-29T14:14:32.791571Z","iopub.execute_input":"2021-06-29T14:14:32.792178Z","iopub.status.idle":"2021-06-29T14:14:34.283297Z","shell.execute_reply.started":"2021-06-29T14:14:32.792134Z","shell.execute_reply":"2021-06-29T14:14:34.282302Z"},"trusted":true},"execution_count":null,"outputs":[]}]}